# ğŸ§  Building GPT-2 (124M) from Scratch â€” A Journey Into LLMs

Hi ğŸ‘‹  

This repo exists because I wanted to *really* understand every nut and bolt of how **large language models (LLMs)** like GPT work.  
So I built the **complete GPT-2 architecture (124M parameters)** entirely **from scratch in PyTorch**, layer by layer â€” no shortcuts, no libraries, just me and `Pytorch`.

And yeah... I trained it **on CPU** ğŸ˜….  
No GPU, no TPU â€” just pure curiosity and stubbornness.  
Learning shouldnâ€™t stop just because hardware does ğŸš€  

Initial training was done on a small storybook dataset called **â€œVerdicâ€**, just to get things working end-to-end.

After training and fine-tuning, I also added **Top-k sampling** and **Temperature scaling** during text generation to improve diversity and control randomness.

---

## âš™ï¸ Overview

This project started as a learning experiment and slowly evolved into a complete, working GPT-2 replica â€”  
from **tokenizer â†’ embeddings â†’ transformer â†’ language model head â†’ training â†’ Sampling Improvements: Top-k & Temperature Scaling â†’ fine-tuning â†’ evaluation**.

Then I didnâ€™t stop there ğŸ˜„  
After verifying my model architecture, I:
- Loaded **OpenAI GPT-2 pretrained weights** into my implementation  
- Did **classification fine-tuning** (spam vs non-spam)  
- Tried **instruction fine-tuning** (Alpaca-style)  
- And finally **evaluated my model** with **ollama** using **Llama-3-2B** as the *external judge*.  

---

## ğŸ“‚ Folder Structure

Hereâ€™s the full repo layout for clarity:

```
Root
â”œâ”€â”€ README.md                # this file
â”‚
â”œâ”€â”€ Code_base/
â”‚   â”œâ”€â”€ 1.Tokenizer/                   # tokenizer experiments (BPE, tokenization notes)
â”‚   â”œâ”€â”€ 2.Dataloader_and_embeddings/   # batching & embeddings pipeline
â”‚   â”œâ”€â”€ 3.E2e_data_processing/         # dataset cleaning and preprocessing notebooks
â”‚   â”œâ”€â”€ 4.Attention/                   # toy and trainable attention notebooks (step-by-step)
â”‚   â”œâ”€â”€ 5.GPT Architechture/           # GPT model implementation, training/evaluation notebooks, plots, checkpoints
â”‚   â””â”€â”€ 6.Finetuning/                  # finetuning notebooks and scripts (classification & instruction tuning)
â”‚
â””â”€â”€ Dataset/
    â”œâ”€â”€ the-verdict.txt                # main training text (storybook)
    â”œâ”€â”€ spam_dataset.csv               # spam/non-spam classification data
    â”œâ”€â”€ instruction_data.json          # Alpaca-style instruction dataset
```

---

## ğŸ—ï¸ Model & Training Details

| Component | Description |
|------------|-------------|
| **Framework** | PyTorch |
| **Model** | GPT-2 Small (124M parameters) |
| **Layers** | 12 Transformer blocks, 12 attention heads, 768 hidden dim |
| **Positional Encoding** | Learned embeddings |
| **Activation** | GELU |
| **Optimizer** | AdamW |
| **Loss Function** | Cross-Entropy |
| **Context Length** | 256 tokens |
| **Vocab Size** | ~50k (GPT-2 BPE compatible) |
| **Hardware** | CPU only (no GPU) |
| **Dataset** | â€œVerdicâ€ storybook text |

Even with limited compute, the model successfully ran through the full training pipeline:
- tokenization  
- batching  
- forward + backward pass  
- weight updates  
- and basic text generation ğŸ‰

---

## ğŸ”§ Fine-Tuning Experiments

### ğŸ§  1. Loading Pretrained GPT-2 Weights
After validating my own GPT-2 architecture, I loaded **OpenAIâ€™s official GPT-2 (124M)** weights into my implementation.  
Layer shapes matched perfectly â€” proving my model replicated the real GPT-2 spec accurately.

---

### ğŸ“¬ 2. Spam / Non-Spam Classification
Once the pretrained weights were loaded, I fine-tuned GPT-2 for a **binary text classification** task.  

**Approach:**
- Froze **all transformer layers** except:
  - Final Transformer block  
  - Final LayerNorm  
  - Classification head (modified MLP â†’ 2 logits)
- Loss: Binary Cross-Entropy  
- Goal: Teach GPT-2 to act as a classifier with minimal fine-tuning.

This was my first test of *repurposing a generative LLM for classification.*

---

### ğŸ’¬ 3. Instruction Fine-Tuning (Alpaca-Style)
Then came the fun part ğŸ˜„  

I formatted custom **instruction-following data** (in the style of Alpaca):

```json
{
  "instruction": "Summarize the paragraph.",
  "input": "Once upon a time...",
  "output": "It tells a story about..."
}
```

Then I fine-tuned the pretrained GPT-2 on this **manually curated dataset** to make it more conversational and capable of following prompts.

**Goal:** Make my small GPT-2 â€œtalkâ€ like an instruction-tuned model.

---

## ğŸ§ª Evaluation

After instruction fine-tuning, I wanted to see **how well my GPT-2-from-scratch model followed instructions** compared to the expected dataset outputs.

For this, I used **OLLa (Open LLM Arena)** with **LLaMA 3.2B** acting as an *LLM judge* â€” it scored my modelâ€™s responses on relevance, correctness, and clarity.

Below are two examples from the evaluation:

---

### ğŸ§¾ Example 1

**Dataset response:**
> The car is as fast as lightning.  

**Model response:**
> The car is as fast as a bullet.  

**Judge (LLaMA 3.2B) score:**
> **85 / 100**

**Reasoning (summarized):**
- Correctly uses a simile structure (â€œas fast as â€¦â€).  
- Comparison is relevant â€” bullets imply high velocity.  
- Slightly less vivid than â€œlightning,â€ but still effective.  

âœ… *A strong, contextually appropriate response showing the model can produce figurative language accurately.*

---

### ğŸ§¾ Example 2

**Dataset response:**
> The type of cloud typically associated with thunderstorms is cumulonimbus.  

**Model response:**
> A thunderstorm is a type of cloud that typically forms in the atmosphere over a region of high pressure...  

**Judge (LLaMA 3.2B) score:**
> **20 / 100**

**Reasoning (summarized):**
- Did **not** answer the actual question.  
- Provided irrelevant or inaccurate meteorological info.  
- Failed to identify â€œcumulonimbusâ€ as the correct answer.  

âŒ *Shows where factual grounding still needs improvement â€” the model drifted off-topic and generated plausible but incorrect text.*

---

These evaluations highlight that while my instruction-tuned GPT-2 can **handle basic analogy and phrasing tasks well**, it still **struggles with factual recall and question precision** â€” common challenges for small LLMs without large-scale pretraining.


## ğŸ’¡ Key Learnings

- Built GPT-2 architecture entirely from scratch â€” no copying Hugging Face code.  
- Understood attention, layer norm, and feed-forward layers *deeply*.  
- Learned why **cross-entropy**, **masking**, and **residual connections** are critical.  
- Saw how easily pretrained models can be **repurposed** with small fine-tuning.  
- And most importantly â€” that even **on CPU**, you can still explore LLMs end-to-end ğŸ’ª  

---

## ğŸ› ï¸ Future Plans

- Add **Rotary embeddings (RoPE)** and compare to learned positional encodings  
- Implement **parameter-efficient fine-tuning (LoRA)**  
- Try **distributed GPU training** for larger experiments   
- Experiment with **text summarization fine-tuning**

---

## ğŸ¤ Acknowledgments
- [Sebastian Raschka Book](https://github.com/rasbt/LLMs-from-scratch)
- ğŸ§¾ [OpenAI GPT-2 paper (2019)](https://openai.com/research/language-unsupervised)
- [OpenAI GPT-2 Open Weights](https://openaipublic.blob.core.windows.net/gpt-2/models) 
- ğŸ§‘â€ğŸ« [Vizuaraâ€™s *â€œLLM from scratchâ€* Playlist](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)  
- ğŸ¤— Hugging Face Transformers (for reference implementations)  

---

## ğŸš€ Final Thought

This project isnâ€™t about beating GPT-2 â€” itâ€™s about **understanding GPT-2**.  
Every line of code here was written, debugged, and tested manually to *see how transformers think*.  

If youâ€™re someone who learns best by building things from scratch â€”  
youâ€™ll feel right at home here â¤ï¸  

---

â­ **If you find this repo interesting, give it a star!**  
Learning should always be open-source ğŸ’ª
